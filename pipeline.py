# By Hussein Mohsen
# Capstone Project in Bioinformatics
# June-December 2014

import sys
import math
import itertools
import os
import random
from random import randrange

# gets arcs of given nodes from graph
def getArcs(fileName, nodes):

	# dictionary of multiplicity of graph arcs and another for arcs between nodes (list representation of graph)
	multipl={}
	graph={}

	# flag that helps in parsing the file	
	detectedArcs=False

	graphFile = open(fileName)

	line = graphFile.readline().strip()

	while 1:
		if not line or (detectedArcs==True and 'ARC' not in line):
			break

		if 'ARC' in line:
			list = line.split('\t')

			fromNode= abs(int(list[1]))
			toNode= abs(int(list[2]))
			multiplicity= int(list[3])

			if fromNode in nodes and toNode in nodes:
				multipl[fromNode, toNode]=multiplicity
				multipl[toNode, fromNode]=multiplicity

				# update degrees
				if fromNode in graph:
					graph[fromNode].append(toNode)
				else:
					graph[fromNode]=[toNode]

				if toNode in graph:
					graph[toNode].append(fromNode)
				else:
					graph[toNode]=[fromNode]

			detectedArcs=True

		line = graphFile.readline()

	#print multipl
	#print graph 

	return multipl, graph

# gets the average multiplicity over arcs of a node
# used to calculate combined score of each node
def getAvMultipl(node, multipl, graph):
	total=0.0

	if node in graph.keys():
		for toNode in graph[node]:
			total+=multipl[node,toNode]

		average=total/len(graph[node])

		return average
	else:
		return 1

# returns node scores. The score of each node is a combined score of average multiplicity and coverage
def getScores(nodes, indices, NodeCovs, NodeLens, multipl, graph):

	sumOfLens = sum(NodeLens)
	scores=[]

	for node in nodes:
		index=indices[node]
		score=NodeCovs[index]+getAvMultipl(node, multipl, graph)
		score=score*(float(NodeLens[index])/sumOfLens)
		#print str(node)+" "+str(score)
		scores.append(score)

	#print scores
	return scores

# gets node numbers and transcripts returns them in a list
# gets confidence and length of each transcript along with nodes that constitutes it
def getNodesAndTranscripts(fileName):

	# lists node numbers 
	nodes=[]

	seqsFile = open(fileName)

	line = seqsFile.readline().strip()

	# parse Nodes
	while 1:
		if not line:
			break

		if line.startswith('>'):
			if 'Node' in line:
				node=int(line[line.rfind('_')+1:])
				nodes.append(node)
			else:
				break

		line = seqsFile.readline().strip()

	# lists for transcripts names, lengths and confidence scores
	TransNames=[]
	TransConfs=[]
	TransLens=[]
	

	# 2D list of nodes per transcript
	TransNodes=[]

	# each row contains nodes in one of the transcripts
	# parse Transcripts
	while 1:
		if not line:
			break

		if line.startswith('>'):

			TransNames.append(line[1:])

			conf = float(line[line.find('Confidence_')+11:line.find('_Length_')])
			length = int(line[line.find('_Length_')+8:])
			
			TransConfs.append(conf)
			TransLens.append(length)

			#print conf
			#print length
		else:
			tempNodes=[]

			tempNodes.append(int(line[0:line.find(':')]))
			
			index = line.find('->')

			while index != -1:
				line=line[index+2:]
				tempNodes.append(abs(int(line[0:line.find(':')])))
				index = line.find('->')

			TransNodes.append(tempNodes)
			
		line = seqsFile.readline().strip()
	

	seqsFile.close()
 
	#print str(len(nodes))+' nodes.'

	nodes.sort()

	# for practical use: to know the index of each node
	indices={}

	for i in range(0, len(nodes)):
		indices[nodes[i]]=i

	for l in TransNodes:
		l.sort()

	#print nodes
	#print indices
	#print TransNodes
	#print TransLens
	#print TransConfs

	return nodes, indices, TransNames, TransNodes, TransLens, TransConfs

# gets the statistics (coverage) from the stats file generated by Oases
def getStatsOfNodes(statsFileName, nodes):
	
	# coverage and length of each node
	NodeCovs=[]
	NodeLens=[]
	
	prevNode=0
	nextNode=nodes[0]

	statsFile = open(statsFileName)
	
	# header
	line = statsFile.readline().strip()

	for i in range(1, len(nodes)+1):
		difference= nextNode-prevNode

		for j in range(0, difference):
			line = statsFile.readline().strip()

		NodeCovs.append(float(line.split('\t')[4]))
		NodeLens.append(int(line.split('\t')[1]))

		if i != len(nodes):
			prevNode=nextNode
			nextNode=nodes[i]

	statsFile.close()

	#print NodeCovs
	#print NodeLens
	#print nodes
	#print len(NodeCovs)

	return NodeCovs, NodeLens

# gets all node and transcript details of Locus parameter-id from contig-ordering.txt
# and exports it to file Locusparameter-id.txt in running directory
def getLocusData(id, contigOrderingFileName):

	# created to read from contig-ordering.txt file
	contigOrderingFile=open(contigOrderingFileName, 'r')

	# create file of locus
	locusFile=open("Locus"+str(id)+".txt", "w")
	
	line = contigOrderingFile.readline().strip()

	# parse Nodes to reach transcripts
	while 1:
		if not line:
			# checking for double not lines to parse the Oases contig-order file perfectly
			line = contigOrderingFile.readline().strip()

			if not line:
				break

		# get Nodes of Locusid
		if line.startswith('>Locus_'+str(id)+'_Node'):		
			while 1:
				if not line or (line.startswith('>')==True and line.startswith('>Locus_'+str(id)+'_Node')==False):
					break

				locusFile.write(line+'\n')

				line = contigOrderingFile.readline().strip()
		
		# get Transcripts of Locusid
		if line.startswith('>Locus_'+str(id)+'_Transcript'):		
			while 1:
				if not line or (line.startswith('>')==True and line.startswith('>Locus_'+str(id)+'_Transcript')==False):
					break

				locusFile.write(line+'\n')

				line = contigOrderingFile.readline().strip()

		line = contigOrderingFile.readline().strip()

	contigOrderingFile.close()

 
# sigmoid function
def sigmoid(x):
  return 1 / (1 + math.exp(-x))

# poisson function
def poisson(x, y):
  return math.exp(-abs(x-y))

# checks for the minimum abs value in the matrix and add it to all cells in matrix
# primarily used to remove any negative values in the matrix, if any
def updateMatrix(matrix):
	
	minVal=min(matrix)

	if minVal<0:
		absMin=abs(minVal)

		for i in range(0, len(matrix)):
			matrix[i]+=(absMin+0.25)

####### STEPS A: For MultiTrans Loci #######

# STEP A1: The EM algorithm
# -------------------------
# N is the number of transcripts (that we assume we know)
def EM(N, nodes, indices, TransNodes, TransLens, TransConfs, NodeCovs, NodeLens, multipl, graph, scores):
	
	# EM theta matrix
	matrix= generateInitialValues(N)

	threshold=0.00001
	difference=0.00001

	MAX_ITER=25
	counter=0
	divFactor=10.0

	# save pre-likelihood values for each iteration
	prelk= [[0 for i in range(N)] for j in range(len(nodes))]

	# save likelihood values for each iteration
	lk= [[0 for i in range(N)] for j in range(len(nodes))]

	while difference>=threshold and counter<MAX_ITER:
		counter+=1

		# E step:
		# calculate likelihood values (those that should sum up to 1: each prelk/total)
		for node in range(0, len(nodes)):
			for trans in range(0, N):
				difference=abs(scores[node]-matrix[trans])/divFactor
				#print "diff of node "+str(node)+ "with trans "+str(trans)+" is: "+str(difference)
				lk[node][trans]=(sigmoid(difference)-0.9)*10
				
		# M step: update matrix

		# to be used to update the values in the matrix below
		# each node's score used to update the value for only one transcript in the matrix
		# which is the transcript with maximum likelihood for the node
		
		maxlk = []

		for node in range (0, len(nodes)):
			max=lk[node][0]
			maxInd=0
			
			for trans in range (1, len(matrix)):
				if lk[node][trans]>max:
					maxInd=trans

			maxlk.append(maxInd)

		for trans in range(0, len(matrix)):
			total=0.0
			numNodes=0.0

			for node in range(0, len(nodes)):
				if trans==maxlk[node]:
					total+=lk[node][trans]*scores[node]
					numNodes+=1

			if numNodes!=0:
				matrix[trans]=total/numNodes

		stir(matrix)

		#print matrix
		#print lk

	updateMatrix(matrix)

	#print matrix
	#print lk

	return matrix

# stirs the matrix by adding sligh random decimals to break ties (if any)
def stir(matrix):
	for i in range(0, len(matrix)):
		addOrsub = random.random()

		if addOrsub<0.5:
			matrix[i]-=random.random()/10
		else:
			matrix[i]+=random.random()/10

	#print matrix
	
# generates initial values in the matrix
def generateInitialValues(N):

	matrix=[]

	if N>1:	
		change = 45.0/(N-1)

		matrix.append(5.0)
	
		value = 5.0

		for i in range(0, N-2):
			value+=change
			matrix.append(value)

	matrix.append(50.0)

	return matrix

# STEP A2: Predict (our) Transcripts
# ----------------------------------
# generates the transcripts based on the matrix obtained by the EM algorithm
def generateTranscripts(matrix, scores, nodes):
	# gets extended matrix based on all combinations
	# retruned 'combinations' is an index: it tells sum of which node(s) in each cell in extended matrix

	eMatrix, combinations = extendMatrix(matrix)

	# initialization of predicted transcripts
	predTransNodes=[]

	for i in range(0, len(matrix)):
		predTransNodes.append([])

	# obtain predicted memberships and build predTransNodes
	for i in range(0, len(nodes)):
		min = sys.float_info.max
		minIndex=-1

		# get the most likely membership(s)
		for j in range(0, len(eMatrix)):
			dist=abs(scores[i]-eMatrix[j])

			if dist<min:
				min=dist
				minIndex=j

		# update predicted predTransNodes
		for k in combinations[minIndex]:
			predTransNodes[k].append(nodes[i])

	#print predTransNodes
	return predTransNodes

# gets all combinations
def extendMatrix(matrix):
	# for combinations having more than one node, a penalty value for each additional one
	multinodePenalty = 19

	eMatrix=[]
	combinations=[]

	alphabet=range(0, len(matrix))

	for i in range(1, len(matrix)+1):
		combos = itertools.combinations(alphabet, i)
	
		for comb in combos:
    			combinations.append(comb)

			sum=0

			for item in comb:
				sum += matrix[item]
			
			# add penalty (if applicable)
			sum += (len(comb)-1)*multinodePenalty

			eMatrix.append(sum)

	#print eMatrix
	#print combinations

	return eMatrix, combinations


# STEP A3 Choose from Oases Transcripts
#---------------------------------------
# predTransNodes are the transcripts (and nodes of each) by our pipeline
# TransNodesare are the transcripts (and nodes of each) by Oases
def filterOasesTranscripts(predTransNodes, TransNodes):
	
	# distance/similarity between pipeline and Oases transcripts
	# each row corresponds to a pipeline-predicted transcript and each column to an Oases transcript
	dist= [[0 for i in range(len(TransNodes))] for j in range(len(predTransNodes))]

	for i in range(0, len(predTransNodes)):
		for j in range(0, len(TransNodes)):
			# get the score(predTransNodes[i], TransNodes[j])	
			dist[i][j]=distance(predTransNodes[i], TransNodes[j])

	#print dist

	# choose the best Oases transcript with minimum distance to each predicted transcripts
	# this will be the final chosen Oases transcripts
	selected= [False for j in range(len(TransNodes))]
	for i in range(0, len(predTransNodes)):
		min=sys.maxint
		minIndex=-1

		for j in range(0, len(TransNodes)):
			if dist[i][j]<min and selected[j]==False:
				# so far this is minimum distance of one of the unselected Oases transcripts
				# save minimum distance values and index of unselected transcript (so far)
				min=dist[i][j]
				minIndex=j

		# select the Oases transcript after for loop has ended and you found what you were looking for
		selected[minIndex]=True

	#print selected
	
	# returns the selected Oases transcripts by the pipeline (ones with True are selected)
	return selected

# calculates distance between two transcripts (each given as a parameter list of nodes)
def distance(trans1, trans2):
	
	common=0
	index1=0
	index2=0

	while index1<len(trans1) and index2<len(trans2):
		if trans1[index1]==trans2[index2]:
			common+=1
			index1+=1
			index2+=1
		elif trans1[index1]>trans2[index2]:
			index2+=1
		else:
			index1+=1
	
	dist=max(len(trans2),len(trans1))-common

	return dist


####### STEP B: For Loci 1 Oases-generated transcript #######
#============================================================
# used in export1TransLociScores() below
def parse1TransFiles(fileName):
	# 1-Trans Loci #s 
	loci=[]

	inputFile = open(fileName)

	line = inputFile.readline().strip()

	while 1:
		if not line:
			break

		loci=loci+line.split(' ')

		line = inputFile.readline().strip()

	inputFile.close()

	return loci

# The commented part exports scores of transcripts in 1-Transcript Loci based on the 1Trans****HitsLoci.txt files generated by export1TransLoci() in blastAll.py
# scores are stored in files parameter-outputDiretory/1Trans****HitsLociScoreDist.txt
# The uncommented part exports distributions of average coverage values of 1Trans-Loci based on the 1Trans****HitsLoci.txt
# Note1: parameter directory is where files 'stats.txt' and 'LastGraph' exist
# NOTE2 (IMP): This helps us determine distribution of scores (plotted as histogram by R) within these 2 types of 1-Trans Loci (loci with 0-Blast hits and >=1 Blast hit transcripts so that our pipeline may filter them)
def export1TransLociScores(zeroFileName, positFileName, resultsDirectory):
	
	# 1-Trans Loci numbers with transcripts of zero BLAST hits then one or more BLAST hits, respectively
	zeroLoci= parse1TransFiles(zeroFileName)
	positLoci= parse1TransFiles(positFileName)
	
	# open file for 1-Trans zero hits distribution
	zeroFile3 = open(resultsDirectory+'res_mergedAssembly/1TransZeroHitsLociCovAvgDist.txt', "w")

	temp3=''

	print "Working on 1TransZeroHitsLociScoreDist.txt..."

	# iterate and generate average scores to file
	for num in zeroLoci:
		print "OneTrans: working on locus z"+str(num)
		getLocusData(int(num), resultsDirectory+'res_mergedAssembly/contig-ordering.txt')
		nodes, indices, TransNames, TransNodes, TransLens, TransConfs= getNodesAndTranscripts("Locus"+str(num)+".txt")
		NodeCovs, NodeLens= getStatsOfNodes(resultsDirectory+'res_mergedAssembly/stats.txt', nodes)

		temp3+= str(sum(NodeCovs)/float(len(nodes)))+' '

		os.remove("Locus"+str(num)+".txt")

	zeroFile3.write(temp3.strip()+'\n')

	zeroFile3.close()
	
	# open file for 1-Trans zero hits distribution
	positFile3 = open(resultsDirectory+'res_mergedAssembly/1TransPositiveHitsLociCovAvgDist.txt', "w")

	temp3=''

	# iterate and generate average scores to file
	for num in positLoci:
		print "OneTrans: working on locus p"+str(num)
		getLocusData(int(num), resultsDirectory+'res_mergedAssembly/contig-ordering.txt')
		nodes, indices, TransNames, TransNodes, TransLens, TransConfs= getNodesAndTranscripts("Locus"+str(num)+".txt")
		NodeCovs, NodeLens= getStatsOfNodes(resultsDirectory+'res_mergedAssembly/stats.txt', nodes)

		temp3+= str(sum(NodeCovs)/float(len(nodes)))+' '

		os.remove("Locus"+str(num)+".txt")

	positFile3.write(temp3.strip()+'\n')

	positFile3.close()


####### STEP C: Reporting results #######
#This is when BLAST results are known. Calculate FP, FN, TP and TN to get other metric values
#===========================================================================================

# Step C1: report results on Mutli-trans Loci
# -------------------------------------------

# returns two dictionaries (for multi-trans loci and one-trans loci) with actual N of each loci based on BLAST hits
# Each N is to be used as a parameter in the EM algorithm for multiTrans or for checking if the only transcript was hit or not for oneTrans Loci
def getNs(lociTransBlastHitsFileName):

	# This dictionary is to be returned. It contains the actual N of each locus ID based on BLAST hits.
	multiTransNs={}
	oneTransNs={}

	lociBlastHitsFile = open(lociTransBlastHitsFileName, 'r')

	line = lociBlastHitsFile.readline().strip()

	## Get locus ID and N: number of actual transcripts
	while 1:
		if not line:
			break

		tokens = line.split()

		# neglect 1-Trans Loci (lines with 3 tokens)

		locusID = int(tokens[1])
	
		# number of Oases transcripts in this Locus that got blast hits
		N = getN(tokens)
		
		if len(tokens)>3:
			multiTransNs[locusID]=N
		else:
			oneTransNs[locusID]=N		

		line = lociBlastHitsFile.readline().strip()

	return multiTransNs, oneTransNs

def getN(tokens):
	N=0

	# tokens start at index 2. Index 0 and 1 are for 'Locus' string and locus ID
	for i in range(2, len(tokens)):
		if tokens[i]!='0':
			N+=1

	return N

# gets the transDict which determines the transcripts that got BLAST hits and how many hits for each transcript
def buildTransDict(directoryList, threshold, transDict):
	for directory in directoryList:
		files=os.listdir(directory)
	
		for file in files:
			if file.endswith('.out'):
				# gets the transcript on first hit in BLAST results
				transNames, eVals, noHits =parseOutFile(directory+file)
	
				for i in range(0, len(transNames)):
					transName=transNames[i]
					eVal=eVals[i]
	
					if transName != 'NO_HITS' and eVal<threshold:
						if transName  in transDict:
		  					transDict[transName]=transDict[transName]+1
						else:
							transDict[transName]=1

# parses a .out file that contains BLAST results
# these files are generated bu blastAll() above
# returns the name and e-value of top hit
# used in getTransDict() above
def parseOutFile(fileName):
	noHits=0

	outFile = open(fileName, "r")

	line = outFile.readline().strip()

	# initialize lists
	names=[]
	EValues=[]

	while 1:
		if "No hits found" in line:
			noHits=noHits+1 
			break

		if "Matrix:" in line:
			break

		if '>' in line:
			names.append(line[1:].strip())

			line = outFile.readline().strip()
			line = outFile.readline().strip()

			# this will contain the e-value (revise BLAST output files)
			line = outFile.readline().strip()

			EValues.append(float(line[line.index("Expect = ")+8:]))

		line = outFile.readline().strip()

	outFile.close()

	return names, EValues, noHits

# Step C1 report results on OneTrns Loci
# -------------------------------------------

def reportMultiTransResults(resultsDirectory, transDict, multiTransNs, allSelected):

	# Total number of transcripts
	totalTrans = 0

	# Total number of Loci that were processed
	totalLoci = len(multiTransNs.keys())

	# True positives and negatives, False positives and negatives
	TP = FP = TN = FN = 0

	# for each multiTrans locus
	for ID in multiTransNs.keys():

		print "MultiTrans: working on Locus "+str(ID)+"..."

		# get Locus data in a file
		getLocusData(ID, resultsDirectory+'res_mergedAssembly/contig-ordering.txt')
	
		fileName='Locus'+str(ID)+'.txt'

		# to save some time on exponential growth of extendMatrix
		if multiTransNs[ID]>10:
			os.remove(fileName)
			totalLoci-=1
			print "Done with Locus "+str(ID)+"..."
			continue


		nodes, indices, TransNames, TransNodes, TransLens, TransConfs = getNodesAndTranscripts(fileName)	

		totalTrans += len(TransNames)

		if multiTransNs[ID]==0:
			TP+=len(TransNames)
			continue

		NodeCovs, NodeLens= getStatsOfNodes(resultsDirectory+'res_mergedAssembly/stats.txt', nodes)
		multipl, graph= getArcs(resultsDirectory+'res_mergedAssembly/LastGraph', nodes)
	
		#print "\nGraph (above) and multiciplity (bottom):\n======================================================="
		#print graph
		#print multipl

		#print "\nScores:\n======================================================="
		scores= getScores(nodes, indices, NodeCovs, NodeLens, multipl, graph)

		#print "\nMatrix:\n======================================================="
		matrix = EM(multiTransNs[ID], nodes, indices, TransNodes, TransLens, TransConfs, NodeCovs, NodeLens, multipl, graph, scores)

		predTransNodes = generateTranscripts(matrix, scores, nodes)

		#print "\nPredicted (above) and Oases (bottom) transcript nodes:\n======================================================="
		#print predTransNodes
		#print TransNodes

		#print "\nSelected Oases transcripts:\n======================================================="
		selected=filterOasesTranscripts(predTransNodes, TransNodes)

		for i in range(0, len(selected)):
			if selected[i] == True:
				allSelected[TransNames[i]] = 1	

		# check if the pipeline selected the "real" transcripts
		for i in range(0, len(selected)):
			if selected[i]==True:				
				try:
					if transDict[TransNames[i]]>0:
						TP+=1
				except KeyError:
					FP+=1
			else:
				try:
					if transDict[TransNames[i]]>0:
						FN+=1
				except KeyError:
					TN+=1
		
		os.remove(fileName)
		print "Done with Locus "+str(ID)+"..."
	
	return totalLoci, totalTrans, TP, TN, FP, FN

# Step C2: report results on OneTrns Loci
# -------------------------------------------

# trains the model using distributions in given files in variable-directories\parameters-inputFiles
# number of buckets to divide the distribution upon 
def trainModel(zeroDistFileName, positiveHistFileName, numOfBuckets, directories):
	# maxValue is the maximum value in the distribution. If coverage, this maxValue should be 60.0, if length it should be 1000.0 
	maxValue = 60.0

	if 'Len' in zeroDistFileName:
		maxValue = 1000.0

	# used to assign each value in distribution to a bucket
	factor = maxValue/numOfBuckets
 
	# records the frequencies in each bucket
	# main model to base predictions on
	zeroHitsBucketFreqs = [0 for i in range(numOfBuckets)]
	positiveHitsBucketFreqs = [0 for i in range(numOfBuckets)]

	# loop over files in all directories
	for dir in directories:
		
		zeroFile = open(dir+zeroDistFileName, "r")
		posiFile = open(dir+positiveHistFileName, "r")

		# update zero model
		line1 = zeroFile.readline().strip()
		
		while 1:
			if not line1:
				break
			
			vals = line1.split()

			for val in vals:
				if float(val) > 10000000000:
					zeroHitsBucketFreqs[numOfBuckets-1]+=1
					continue

				bucket = int(math.floor(float(val)/factor))

				if bucket>= numOfBuckets:
					zeroHitsBucketFreqs[numOfBuckets-1]+=1
				else:
					zeroHitsBucketFreqs[int(math.floor(float(val)/factor))]+=1

			line1 = zeroFile.readline().strip()

		# update positive model
		line2 = posiFile.readline().strip()
		
		while 1:
			if not line2:
				break
			
			vals = line2.split()

			for val in vals:
				if float(val) > 10000000000:
					positiveHitsBucketFreqs[numOfBuckets-1]+=1
					continue

				bucket = int(math.floor(float(val)/factor))

				if bucket>= numOfBuckets:
					positiveHitsBucketFreqs[numOfBuckets-1]+=1
				else:
					positiveHitsBucketFreqs[int(math.floor(float(val)/factor))]+=1

			line2 = posiFile.readline().strip()

	print zeroHitsBucketFreqs
	print positiveHitsBucketFreqs

	return zeroHitsBucketFreqs, positiveHitsBucketFreqs, maxValue

# This function runs after TrainModel and accept/reject 1TransLoci based on (1) its score and (2) zeroHitsBucketFreqs and  positiveHitsBucketFreqs by TrainModel
# oneTransNs is a dictionary whose keys are Loci IDs with 1 transcript and values the number of BLAST hits got by this transcript: N>=0
# Parameter type is type of distribution. Type 1: average coverage, 2: average length
# Parameter inverseThreshold: if length of transcript > inverseThreshold, it is not considered.
def reportOneTransResultsPerDist(oneTransNs, positiveHitsBucketFreqs, zeroHitsBucketFreqs, maxValue, OasesResultsDirectory, type, transDict, inverseThreshold, allSelected):

	# average length of transcript under consideration
	avgLen = 0 

	# Total number of Loci that were processed
	totalLoci = len(oneTransNs.keys())

	# Total number of transcripts
	# Since it's 1 trans/locus, then totalTrans=totalLoci
	totalTrans = totalLoci

	# used to assign each value in distribution to a bucket
	# zeroHitsBucketFreqs and positiveHitsBucketFreqs have same length of course
	factor = maxValue/len(zeroHitsBucketFreqs)

	# True positives and negatives, False positives and negatives
	TP = FP = TN = FN = 0

	# for each multiTrans locus
	for ID in oneTransNs.keys():
		print "OneTrans: working on Locus "+str(ID)+"..."

		# get Locus data in a file
		getLocusData(ID, OasesResultsDirectory+'/contig-ordering.txt')
	
		fileName='Locus'+str(ID)+'.txt'
		nodes, indices, TransNames, TransNodes, TransLens, TransConfs = getNodesAndTranscripts(fileName)

		NodeCovs, NodeLens= getStatsOfNodes(OasesResultsDirectory+'stats.txt', nodes)

		avgLen =  float(sum(NodeLens))/len(NodeLens)

		# average coverage
		if type == 1:
			value = float(sum(NodeCovs))/len(NodeCovs)
		# average length
		elif type == 2:
			value = avgLen

		# for outliers with very high values more than max (shrink them to last bucket)
		if float(value) >= maxValue:
			bucket=len(zeroHitsBucketFreqs)-1
		# for values within mainstream range
		else:
			bucket = int(math.floor(float(value)/factor))

		# now we predict whether to include/exclude this transcript based on the bucket it belongs to
		randomNumber = randrange(zeroHitsBucketFreqs[bucket]+positiveHitsBucketFreqs[bucket]+1)

		# INCLUDE TRANSCRIPT
		if (randomNumber <= (float(zeroHitsBucketFreqs[bucket])*2.0)):			
			try:
				allSelected[TransNames[0]] = 1

				if transDict[TransNames[0]]>0:
					TP+=1
			except KeyError:
					FP+=1
		# EXCLUDE TRANSCRIPT
		else:
			try:
				if transDict[TransNames[0]]>0:
					FN+=1
			except KeyError:
				TN+=1

		os.remove(fileName)
		print "Done with Locus "+str(ID)+"..."

	#print '\n=========\n 1Trans Results:\n========='
	#reportStats(totalLoci, totalTrans, TP, TN, FP, FN)

	return totalLoci, totalTrans, TP, TN, FP, FN

# this methods takes TP, TN, FP, FN and report accuracy and precision
def reportStats(totalLoci, totalTrans, TP, TN, FP, FN):
	
	stats = ''
	
	accuracy = float(TP+TN)/(TP+TN+FP+FN)
	stats += "Accuracy: "+str(accuracy)+'\n'

	precision = float(TP)/(TP+FP)
	stats += "Precision: "+str(precision)+'\n'

	return stats

# generates the filtered assembly file after running the pipeline.
def generateFilteredAssembly(allSelected, resultsDirectory):
	
	# Original Oases assembly
	OasesAssemblyFile = open(resultsDirectory+'res_mergedAssembly/transcripts.fa')

	filteredAssemblyFile = open(resultsDirectory+'res_mergedAssembly/pipelineFilteredAssembly.fa', 'w')

	line = 'initial'

	while 1:
		try:
			if not line:
				break

			if line.startswith('>') and allSelected[line[1:]]==1 :
				while 1:
					filteredAssemblyFile.write(line+'\n')
					line = OasesAssemblyFile.readline().strip()

					if line.startswith('>') or not line:
						break
			else:
				line = OasesAssemblyFile.readline().strip()

		except KeyError:
			line = OasesAssemblyFile.readline().strip()
			continue

	OasesAssemblyFile.close()
	filteredAssemblyFile.close()

# main method
def main():

	if __name__ == "__main__":
		try:
			# get directory where files 'stats.txt' and 'LastGraph' exist
			index=sys.argv.index('-d')
		
			resultsDirectory = str(sys.argv[index+1])

			# check for the number of transcripts files used in blastAll previously
			index=sys.argv.index('-bls')
			bls = int(sys.argv[index+1])

			# check for batch of loci mode
			index=sys.argv.index('-b')

			### BATCH mode ###
			# Report results

			print 'Working in batch mode...'			
			
			# distribution files built
			export1TransLociScores(resultsDirectory+"res_mergedAssembly/1TransZeroHitsLoci.txt", resultsDirectory+"res_mergedAssembly/1TransPositiveHitsLoci.txt", resultsDirectory)
			print "distribution files created and filled."

			# create and update transDict

			blastAllDirectories = []

			for i in range(1, bls+1):
				blastAllDirectory = resultsDirectory+'blastAllResults'+str(i)+'/'
				blastAllDirectories.append(blastAllDirectory)

			transDict={}
			buildTransDict(blastAllDirectories, 1e-30, transDict)

			print "transDict is built."

			# get actual Ns of loci (based on BLAST hits stored in LociTransBlastHits.txt)
			# for each category: multiTransLoci and oneTransLoci
			multiTransNs, oneTransNs = getNs(resultsDirectory+'res_mergedAssembly/LociTransBlastHits.txt')
	
			print "multiTransNs and oneTransNs are obtained."

			# directories from which training data can be found
			# training data is one or more distributions
			directories = [resultsDirectory+'res_mergedAssembly/']

			# Pipeline 1Trans results
			zeroHitsBucketFreqs, posHitsBucketFreqs, maxValue = trainModel('1TransZeroHitsLociCovAvgDist.txt', '1TransPositiveHitsLociCovAvgDist.txt', 10, directories)
			print "1TransLoci Model trained."

			# A dictionary to determine all selected transcripts. It will be used to generate the filtered assembly file last.
			allSelected = {}

			# Sampling 1Trans results
			ototalLoci, ototalTrans, oTP, oTN, oFP, oFN = reportOneTransResultsPerDist(oneTransNs, zeroHitsBucketFreqs, posHitsBucketFreqs, maxValue, resultsDirectory+'res_mergedAssembly/', 1, transDict, 0, allSelected)

			print "1TransNs sampling done."

			# Pipeline multiTrans results
			mtotalLoci, mtotalTrans, mTP, mTN, mFP, mFN = reportMultiTransResults(resultsDirectory, transDict, multiTransNs, allSelected)
	
			print "multiTrans pipeline done."

			# Total results
			print '\n=========\n Combined Results:\n========='
			finalStats = reportStats(mtotalLoci+ototalLoci, mtotalTrans+ototalTrans, mTP+oTP, mTN+oTN, mFP+oFP, mFN+oFN)
			print finalStats

			# Generation of filtered assembly file
			print 'generating filtered assembly file...'			
			generateFilteredAssembly(allSelected, resultsDirectory)
			print '\nDone.'
			
		except ValueError:
			try:
				# check for single locus mode
				index=sys.argv.index('-s')
				
				### SINGLE mode ###
				print 'Working in single mode...'

				# get input file that contains locus i
				index=sys.argv.index('-i')
		
				locusID = int(sys.argv[index+1])

				# get Locus data in a file
				getLocusData(locusID, resultsDirectory+'res_mergedAssembly/contig-ordering.txt')

				fileName = "Locus"+str(locusID)+".txt"
 
				# get F, the number of filtered transcripts
				index=sys.argv.index('-F')

				F = int(sys.argv[index+1])

				if F < 2:
					return


				nodes, indices, TransNames, TransNodes, TransLens, TransConfs = getNodesAndTranscripts(fileName)	
		
				NodeCovs, NodeLens= getStatsOfNodes(resultsDirectory+'res_mergedAssembly/stats.txt', nodes)
				multipl, graph= getArcs(resultsDirectory+'res_mergedAssembly/LastGraph', nodes)

				scores= getScores(nodes, indices, NodeCovs, NodeLens, multipl, graph)

				matrix = EM(F, nodes, indices, TransNodes, TransLens, TransConfs, NodeCovs, NodeLens, multipl, graph, scores)

				predTransNodes = generateTranscripts(matrix, scores, nodes)

				print "\nSelected Oases transcripts:\n======================================================="
				selected = filterOasesTranscripts(predTransNodes, TransNodes)
				print selected
			
				print '\nDone.'
			except ValueError:
				print 'Wrong command.'


main()
